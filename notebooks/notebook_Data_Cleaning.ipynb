{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: Amazon Books Reviews\n",
    "# Author: Mohamed Bekheet\n",
    "# Source: Mohamed Bekheet. (2022). Amazon Books Reviews [Data set]. Kaggle. https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews\n",
    "# URL: https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews\n",
    "# This dataset contains 2 files\n",
    "# The first file** reviews** file contain feedback about 3M user on 212404 unique books the data set is \n",
    "# part of the Amazon review Dataset it contains product reviews and metadata from Amazon, including \n",
    "# 142.8 million reviews spanning May 1996 - July 2014.\n",
    "# Reviews Dataset Data Dictionary:\n",
    "# -id:                  The Id of Book\n",
    "# -Title:   \t        Book Title\n",
    "# -Price:               The price of Book\n",
    "# -User_id:             Id of the user who rates the book\n",
    "# -profileName:         Name of the user who rates the book\n",
    "# -review/helpfulness:  helpfulness rating of the review, e.g. 2/3\n",
    "# -review/score:        rating from 0 to 5 for the book\n",
    "# -review/time:         time of given the review\n",
    "# -review/summary:      the summary of a text review\n",
    "# -review/text:         the full text of a review\n",
    "\n",
    "# The second file Books Details file contains details information about 212404 unique books it file is built by using\n",
    "# google books API to get details information about books it rated in the first file\n",
    "# and this file contains\n",
    "\n",
    "# Book Details Dataset Data Dictionary:\n",
    "# Title:\t        Book Title\n",
    "# Descripe:\t        Decription of book\n",
    "# authors:\t        Name of book authors\n",
    "# image:\t        Url for book cover\n",
    "# previewLink:\t    Link to access this book on google Books\n",
    "# publisher:\t    Name of the publisheer\n",
    "# publishedDate:\tThe date of publish\n",
    "# infoLink:\t        Link to get more information about the book on google books\n",
    "# categories:\t    Genres of books\n",
    "# ratingsCount:\t    Averaging rating for book\n",
    "\n",
    "# Both Datasets are linked through the unique tittle of the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0. Load libraries and custom modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "from dateutil.parser import parse\n",
    "from datetime import datetime\n",
    "# ------------  PREPROCESING -------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#-------------- TRANSFORMERS --------------\n",
    "import transformers\n",
    "from transformers.pipelines import PIPELINE_REGISTRY\n",
    "from transformers import pipeline\n",
    "import evaluate\n",
    "from evaluate import load\n",
    "from transformers import Conversation\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the Datasets\n",
    "df_rating_raw = pd.read_csv('../data/raw/Books_rating.csv')\n",
    "\n",
    "df_rating_raw.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rating_raw[df_rating_raw['Title'] == 'West Side story']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rating_raw.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Books Data Dataset\n",
    "df_data_raw = pd.read_csv('../data/raw/books_data.csv')\n",
    "\n",
    "df_data_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_raw.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Revisar Valores Nulos\n",
    "df_rating_raw.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_raw.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Innecesary Columns are removed\n",
    "# df_rating: \n",
    "#   - Price: Almost 84% of the entries are null. \n",
    "#   - profileName: User_id provides same information.\n",
    "#   - Id: Innecesary column.\n",
    "#   - review/summary: Unncesary column for current analysis. Many summaries used are the same Book Title.  \n",
    "# df_data:\n",
    "#   - image, previewLink, infoLink: Not useful information.\n",
    "#   - ratingsCount: Information from Google API. Not useful for the analysis.\n",
    "#   - description: Non necesary information for Analysis\n",
    "\n",
    "df_rating_processed = df_rating_raw.drop(['Id','profileName','Price','review/summary'], axis=1).copy()\n",
    "df_data_processed = df_data_raw.drop(['image','previewLink','infoLink','ratingsCount','description'], axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rating_processed.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rating_processed = df_rating_processed.rename(columns={'Title':'title', \n",
    "                                                        'User_id':'user_id',\n",
    "                                                        'review/helpfulness': 'helpfulness',\n",
    "                                                        'review/score': 'review',\n",
    "                                                        'review/time': 'review_time',\n",
    "                                                        'review/text': 'text'\n",
    "                                                        })\n",
    "df_data_processed = df_data_processed.rename(columns={'Title':'title', \n",
    "                                                        'publishedDate':'published_date'\n",
    "                                                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rating_processed.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_processed.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEAN THE DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As both datasets are going to be merged by Title, Rows with Null Title are removed.\n",
    "df_rating_processed = df_rating_processed.dropna(subset=['title', 'text'], how='any')\n",
    "\n",
    "df_data_processed = df_data_processed.dropna(subset=['title'])\n",
    "\n",
    "#Drop rows where everything except title is null\n",
    "df_data_processed = df_data_processed.dropna(subset=['authors', 'publisher', 'published_date', 'categories'], how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_processed.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Found 143 published_date entries with the format YYYY*\n",
    "# Found 76 published_date entries with the format YYY?\n",
    "# Found 52 published_date entries with the format YY??\n",
    "\n",
    "def convert_to_year(date_in):    \n",
    "    try:    \n",
    "        patern_1 = r'\\d\\d\\d\\?'\n",
    "        patern_2 = r'\\d\\d\\?\\?'\n",
    "\n",
    "        #Some dates only contain the Year with the format YYYY*. For those cases, the symbol * is removed.\n",
    "        date = date_in.replace('*', '')\n",
    "\n",
    "        #Other cases contains an ? symbol instead of a number on the Year. YYY? or YY??\n",
    "        #For this specific case, the rounded mean from 0 to 9 is used: 5\n",
    "        if re.match(patern_1, date):\n",
    "            date = date.replace('?', '5')\n",
    "\n",
    "        if re.match(patern_2, date):\n",
    "            return None\n",
    "\n",
    "        date = parse(date)\n",
    "        \n",
    "        return date.strftime(\"%Y\")\n",
    "    except:        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use only the year for published_date\n",
    "df_data_processed['published_year'] = df_data_processed['published_date'].apply(lambda x: convert_to_year(x) if pd.notna(x) else x)\n",
    "\n",
    "df_data_processed['published_year'] = df_data_processed['published_year'].fillna('0')\n",
    "\n",
    "df_data_processed['published_year'] = df_data_processed['published_year'].astype('int32')\n",
    "\n",
    "#Drop published_date column \n",
    "df_data_processed = df_data_processed.drop(['published_date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_published_year = int(df_data_processed['published_year'].mean())\n",
    "mean_published_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_processed['published_year'] = df_data_processed['published_year'].apply(lambda x: mean_published_year if x==0 else x)\n",
    "df_data_processed.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_string_to_list(s):\n",
    "    if isinstance(s, str):\n",
    "        return [item.strip(\" '[]\") for item in s.split(',')]\n",
    "    else:\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating the Authors to a list.\n",
    "#Updating Categories to list\n",
    "df_data_processed['authors'] = df_data_processed['authors'].apply(custom_string_to_list)\n",
    "df_data_processed['categories'] = df_data_processed['categories'].apply(custom_string_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_processed.sample(10, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rating_processed.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING DATA FOR REVIEWS DATASET\n",
    "df_rating_processed['review_year'] = df_rating_processed['review_time'].apply(lambda x: datetime.utcfromtimestamp(x).year)\n",
    "\n",
    "df_rating_processed = df_rating_processed.drop(['review_time'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rating_processed.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing helpfulness to a porcentual value\n",
    "def get_helpfulness_as_porcentual_value_0(x):\n",
    "    if  x == '0/0':\n",
    "        return 0\n",
    "    elif x.split('/')[0] == '0':\n",
    "        return 0\n",
    "    else:\n",
    "        dividend = int(x.split('/')[0])\n",
    "        divisor = int(x.split('/')[1])\n",
    "        return dividend/divisor\n",
    "    \n",
    "def get_helpfulness_as_porcentual_value_1(x):\n",
    "    if  x == '0/0':\n",
    "        return 1\n",
    "    elif x.split('/')[0] == '0':\n",
    "        return 0\n",
    "    else:\n",
    "        dividend = int(x.split('/')[0])\n",
    "        divisor = int(x.split('/')[1])\n",
    "        return dividend/divisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rating_processed['helpfulness_0'] = df_rating_processed['helpfulness'].apply(lambda x: get_helpfulness_as_porcentual_value_0(x))\n",
    "df_rating_processed['helpfulness_1'] = df_rating_processed['helpfulness'].apply(lambda x: get_helpfulness_as_porcentual_value_1(x))\n",
    "df_rating_processed = df_rating_processed.drop(['helpfulness'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rating_processed.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge The Two Datasets.\n",
    "df_all_data_processed = pd.merge(df_rating_processed, df_data_processed, on='title', how='inner')\n",
    "df_all_data_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data_processed.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Reduction/Sampling\n",
    "\n",
    "Sample Size Calculation formula from Survey Monkey (https://www.surveymonkey.com/mp/sample-size-calculator/) is used to calculate a extract a ramdom sample of the dataset.\n",
    "In order to reduce the dataset size and apply NLP to the text column.\n",
    "\n",
    "For the given formula: \n",
    "\n",
    "   $$ sample size =  \\frac{\\frac{z^2xp(1-p)}{e^2}}{1 + \\frac{z^2xp(1-p)}{e^2N}} $$\n",
    "\n",
    "with:\n",
    "- N = population size \n",
    "- e = Margin of error (percentage in decimal form) \n",
    "- z = z-score\n",
    "- p = sample proportion\n",
    "\n",
    "\n",
    "| Desired confidence level\t| z-score |\n",
    "|--------------------------|---------|\n",
    "|            80%           |   1.28  |\n",
    "|            85%\t         |   1.44  |\n",
    "|            90%\t         |   1.65  |\n",
    "|            95%\t         |   1.96  |\n",
    "|            99%\t         |   2.58  |\n",
    "\n",
    "\n",
    "Setting N = 2 666 313, e = 99% and z = 2.58\n",
    "\n",
    "sample size = 578995. \n",
    "\n",
    "A sample of **578 995** entries will be extracted from the Dataset to be processed and to train and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = df_all_data_processed.shape[0] - 1\n",
    "e = 0.0015 #Margin of error = 0.15%\n",
    "z = 2.58 # Desired confidence level = 99%, so z-score = 2.58\n",
    "p = 0.5 #Used 0.5 as a conservative approach. It will give the largest sample size\n",
    "\n",
    "sample_size = ( ((z*z)*p*(1-p))/(e*e) )/(1 + ((z*z)*p*(1-p))/(e*e*N) )\n",
    "\n",
    "sample_size = round(sample_size)\n",
    "\n",
    "print(f'Sample size is: {sample_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the sample of 578995 entries from the dataset.\n",
    "df_all_data_filtered_processed = df_all_data_processed.sample(n=sample_size, random_state=2125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data_filtered_processed.reset_index(drop=True, inplace=True)\n",
    "df_all_data_filtered_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data_filtered_processed['review'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data_filtered_processed['review'].hist()\n",
    "plt.show()\n",
    "#Add axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data_filtered_processed.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data_filtered_processed.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data_filtered_processed = df_all_data_filtered_processed.drop_duplicates(subset=['title','user_id', 'review', 'text'])\n",
    "df_all_data_filtered_processed.reset_index(drop=True, inplace=True)\n",
    "df_all_data_filtered_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data_filtered_processed.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Preprocesing from EDA Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpfulness Outliers\n",
    "\n",
    "During the EDA, it was discovered two entries Outliers for helpfulness column with helpfulness = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data_filtered_processed['helpfulness_0'].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# During the EDA, it was discovered a single Outlier for helpfulness column\n",
    "df_all_data_filtered_processed['helpfulness_1'].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data_filtered_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data_filtered_processed = df_all_data_filtered_processed.drop(df_all_data_filtered_processed[df_all_data_filtered_processed['helpfulness_0'] == 2].index)\n",
    "df_all_data_filtered_processed[df_all_data_filtered_processed['helpfulness_0'] == 2]['helpfulness_0'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data_filtered_processed.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data_filtered_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data_filtered_processed['helpfulness_1'].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data_filtered_processed['helpfulness_0'].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors Names Standarization\n",
    "\n",
    "During the Exploratory Data Analysis it was discovered that some authors are reviewed under diferent formats of their names. Based on this, the Dataset is simplified to contain only one name format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The examples are:\n",
    "author_mapping = {\n",
    "    'John Ronald Reuel Tolkien':'J. R. R. Tolkien',\n",
    "    'J. R. R. Tolkien':'J. R. R. Tolkien',\n",
    "    'J.R.R. Tolkien':'J. R. R. Tolkien',\n",
    "    'Joseph Smith':'Joseph Smith',\n",
    "    'Joseph Smith (Jr.)':'Joseph Smith',\n",
    "    'Jr.':'Joseph Smith',\n",
    "    'Gabriel Garcia Marquez':'Gabriel Garcia Marquez',\n",
    "    'Gabriel García Márquez':'Gabriel Garcia Marquez',\n",
    "    'Charlotte Brontë':'Charlotte Brontë',\n",
    "    'Charlotte Bronte':'Charlotte Brontë',\n",
    "    'Emily Brontë':'Emily Brontë',\n",
    "    'Emily Bronte':'Emily Brontë',\n",
    "    'Joseph Smith':'Joseph Smith',\n",
    "    'Joseph Smith (Jr.)':'Joseph Smith',\n",
    "    'Jr.':'Joseph Smith'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data_filtered_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_authors(authors):\n",
    "    if isinstance(authors, list):\n",
    "        return [author_mapping.get(author, author) for author in authors]\n",
    "    else:\n",
    "        return authors  # Return the original value if it's not a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data_filtered_processed['authors'] = df_all_data_filtered_processed['authors'].apply(standardize_authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the Authors Feature to Categorical, and applying Pareto Principe to reduce the quantity of authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_counts = df_all_data_filtered_processed['authors'].explode().value_counts()\n",
    "author_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_author_counts = author_counts.sort_values(ascending=False)\n",
    "sorted_author_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_threshold = int(0.2 * len(sorted_author_counts))\n",
    "pareto_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_authors = sorted_author_counts.iloc[:pareto_threshold].index\n",
    "top_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data_filtered_processed['authors'] = df_all_data_filtered_processed['authors'].apply(lambda x: [author if author in top_authors else 'Other' for author in x] if isinstance(x, list) else x)\n",
    "df_all_data_filtered_processed['authors'].explode().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cateogories Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the Categories Feature to Categorical, and applying Pareto Principe to reduce the quantity of Categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_counts = df_all_data_filtered_processed['categories'].explode().value_counts()\n",
    "categories_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_category_counts = categories_counts.sort_values(ascending=False)\n",
    "sorted_category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_threshold = int(0.2 * len(sorted_category_counts))\n",
    "pareto_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_categories = sorted_category_counts.iloc[:pareto_threshold].index\n",
    "top_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data_filtered_processed['categories'] = df_all_data_filtered_processed['categories'].apply(lambda x: [category if category in top_categories else 'Other' for category in x] if isinstance(x, list) else x)\n",
    "df_all_data_filtered_processed['categories'].explode().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processed Datasets Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data_filtered_processed = df_all_data_filtered_processed.reset_index()\n",
    "df_all_data_filtered_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A reduced Dataset is created to process Text Column and Apply sentiment Analysis.\n",
    "df_all_data_filtered_processed['num_row'] = np.arange(len(df_all_data_filtered_processed))\n",
    "\n",
    "df_reduced = df_all_data_filtered_processed[['num_row', 'text']].copy()\n",
    "\n",
    "df_reduced.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data_filtered_processed.to_csv('../data/processed/filtered_data_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.to_csv('../data/processed/df_reduced.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
