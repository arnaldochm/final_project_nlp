{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "9ZZpOHZExcMw"
      },
      "outputs": [],
      "source": [
        "# Step 0. Load libraries and custom modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "from dateutil.parser import parse\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "# ------------  PREPROCESING -------------\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import FastICA\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk import download\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "#-------------- TRANSFORMERS --------------\n",
        "import transformers\n",
        "from transformers.pipelines import PIPELINE_REGISTRY\n",
        "from transformers import pipeline\n",
        "import evaluate\n",
        "from evaluate import load\n",
        "from transformers import Conversation\n",
        "transformers.logging.set_verbosity_error()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rx0-j5xbxcMx",
        "outputId": "8fde96c4-b40b-4503-91f2-83b620613f84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 15738 entries, 0 to 15737\n",
            "Data columns (total 3 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   Unnamed: 0  15738 non-null  int64 \n",
            " 1   num_row     15738 non-null  int64 \n",
            " 2   text        15738 non-null  object\n",
            "dtypes: int64(2), object(1)\n",
            "memory usage: 369.0+ KB\n"
          ]
        }
      ],
      "source": [
        "df_reduced = pd.read_csv('../data/processed/df_reduced.csv')\n",
        "df_reduced.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "hI-V-oHmxcMy"
      },
      "outputs": [],
      "source": [
        "df_reduced = df_reduced.drop(['Unnamed: 0'], axis= 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "AJnhviSgxcMy",
        "outputId": "642af037-c9b0-43f7-f053-e1bd64234f83"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num_row</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5947</th>\n",
              "      <td>5947</td>\n",
              "      <td>Ice Drift is a story written by Theodore Taylo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11720</th>\n",
              "      <td>11720</td>\n",
              "      <td>This story of Alex Cross takes up right where ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8253</th>\n",
              "      <td>8253</td>\n",
              "      <td>I'm an advanced photography student and i use ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3537</th>\n",
              "      <td>3537</td>\n",
              "      <td>Lewis a conservative Christian put together th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15101</th>\n",
              "      <td>15101</td>\n",
              "      <td>I bought this book out of curiosity. I am a me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10908</th>\n",
              "      <td>10908</td>\n",
              "      <td>I recently purchased his \"The Taming the the S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15611</th>\n",
              "      <td>15611</td>\n",
              "      <td>So great learning all about The Amish Heritage...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14693</th>\n",
              "      <td>14693</td>\n",
              "      <td>This book is well written and hilarious. I rec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8055</th>\n",
              "      <td>8055</td>\n",
              "      <td>What I liked:* Lots of great pictures and illu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11439</th>\n",
              "      <td>11439</td>\n",
              "      <td>I have enjoyed the entire Mitford Years series...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       num_row                                               text\n",
              "5947      5947  Ice Drift is a story written by Theodore Taylo...\n",
              "11720    11720  This story of Alex Cross takes up right where ...\n",
              "8253      8253  I'm an advanced photography student and i use ...\n",
              "3537      3537  Lewis a conservative Christian put together th...\n",
              "15101    15101  I bought this book out of curiosity. I am a me...\n",
              "10908    10908  I recently purchased his \"The Taming the the S...\n",
              "15611    15611  So great learning all about The Amish Heritage...\n",
              "14693    14693  This book is well written and hilarious. I rec...\n",
              "8055      8055  What I liked:* Lots of great pictures and illu...\n",
              "11439    11439  I have enjoyed the entire Mitford Years series..."
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_reduced.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "TnFxeo9YCd1n"
      },
      "outputs": [],
      "source": [
        "# def clean_stopwords(text: str,stop_dict: dict)->str:\n",
        "#     if text is not None:\n",
        "#         words = text.split()\n",
        "#         words_clean = []\n",
        "#         for word in words:\n",
        "#             if word not in stop_dict:\n",
        "#                 words_clean.append(word)\n",
        "#         result = ' '.join(words_clean)\n",
        "#     else:\n",
        "#         result = None\n",
        "#     return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "OaZQCL1hCHjS"
      },
      "outputs": [],
      "source": [
        "# 3.10 Text To Lower\n",
        "df_reduced['text_clean'] = df_reduced['text'].str.lower()\n",
        "# 3.12 Extract special characters and numbers\n",
        "df_reduced['text_clean'] = df_reduced['text_clean'].str.replace(r'[^a-z]', ' ', regex=True)\n",
        "# 3.13 Extract numbers\n",
        "# df_reduced['text_clean'] = df_reduced['text_clean'].str.replace(r'[\\d]+', '', regex=True)\n",
        "# 3.14 #Change multiple white spaces to a single white space\n",
        "df_reduced['text_clean'] = df_reduced['text_clean'].str.replace(r'\\s+',' ',regex=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Z8J38661CzTx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /home/arnaldochm/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# 3.15 Lemmatize Text and removing Stopwords\n",
        "\n",
        "download(\"wordnet\")\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "rObXY_twC7C3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/arnaldochm/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "download(\"stopwords\")\n",
        "stop_words = stopwords.words(\"english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lemmatize_text(words, lemmatizer = lemmatizer):\n",
        "    words = words.split(' ')\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in words]\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    tokens = [word for word in tokens if len(word) > 3]\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10055    unlike reviewer think oprah compilation three ...\n",
              "7394     gave star based general subject appeal enterta...\n",
              "10719    western world must read gritty ground level fi...\n",
              "9326     part wanted story royal family confinement hou...\n",
              "11210    tempted favor pick bookstore read page earlier...\n",
              "2243     great collection clever accomplished song abso...\n",
              "13701    disappointed book really full story people dre...\n",
              "11311    frank peretti know grab reader latest novel go...\n",
              "14731    light reading enjoyed lord ring like work char...\n",
              "1806     typical vampire book reason really enjoyed rob...\n",
              "Name: text_clean, dtype: object"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_reduced[\"text_clean\"] = df_reduced[\"text_clean\"].apply(lambda x: lemmatize_text(x))\n",
        "df_reduced[\"text_clean\"].sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "UDVhJZmmJWPM"
      },
      "outputs": [],
      "source": [
        "df_reduced = df_reduced.drop(['text'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "lvYNEGJPDjLI",
        "outputId": "b9b25394-1a28-4218-c34f-37341d001f42"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num_row</th>\n",
              "      <th>text_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>purchased book neice cruised week loved story ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>really enjoyed book several front author terri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>interesting informative spot good book read wa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>must learn past history better appreciate toda...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>began book high hope cover blurb sounded inter...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>gary demar succeeded writing finest short actu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>first miss julia book found quite delightful b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>enjoying read assuming writes like think long ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>book simply great start reading able page page...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>hobbit written differently lord ring time much...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   num_row                                         text_clean\n",
              "0        0  purchased book neice cruised week loved story ...\n",
              "1        1  really enjoyed book several front author terri...\n",
              "2        2  interesting informative spot good book read wa...\n",
              "3        3  must learn past history better appreciate toda...\n",
              "4        4  began book high hope cover blurb sounded inter...\n",
              "5        5  gary demar succeeded writing finest short actu...\n",
              "6        6  first miss julia book found quite delightful b...\n",
              "7        7  enjoying read assuming writes like think long ...\n",
              "8        8  book simply great start reading able page page...\n",
              "9        9  hobbit written differently lord ring time much..."
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 3.14 See the results\n",
        "df_reduced.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     /home/arnaldochm/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('vader_lexicon')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "vaderSentimentAnalyzer = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'goethe expressed fundamental truth nothing much philosophy footnote plato plotinus year later plato eliminated platonic dabbling politics quietly separated greek mysticism emerging fundamentalism christianity became somewhat bemusedly cult figure right brian hines deal subject hugely sympathatic hope future something change time plato century close timeless truth time well according plotinus father robin'"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_reduced.iloc[67]['text_clean']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'neg': 0.0, 'neu': 0.829, 'pos': 0.171, 'compound': 0.835}"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vaderSentimentAnalyzer.polarity_scores(df_reduced.iloc[67]['text_clean'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num_row</th>\n",
              "      <th>text_clean</th>\n",
              "      <th>scores</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>purchased book neice cruised week loved story ...</td>\n",
              "      <td>{'neg': 0.0, 'neu': 0.647, 'pos': 0.353, 'comp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>really enjoyed book several front author terri...</td>\n",
              "      <td>{'neg': 0.116, 'neu': 0.564, 'pos': 0.32, 'com...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>interesting informative spot good book read wa...</td>\n",
              "      <td>{'neg': 0.0, 'neu': 0.504, 'pos': 0.496, 'comp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>must learn past history better appreciate toda...</td>\n",
              "      <td>{'neg': 0.094, 'neu': 0.554, 'pos': 0.352, 'co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>began book high hope cover blurb sounded inter...</td>\n",
              "      <td>{'neg': 0.094, 'neu': 0.643, 'pos': 0.263, 'co...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   num_row                                         text_clean  \\\n",
              "0        0  purchased book neice cruised week loved story ...   \n",
              "1        1  really enjoyed book several front author terri...   \n",
              "2        2  interesting informative spot good book read wa...   \n",
              "3        3  must learn past history better appreciate toda...   \n",
              "4        4  began book high hope cover blurb sounded inter...   \n",
              "\n",
              "                                              scores  \n",
              "0  {'neg': 0.0, 'neu': 0.647, 'pos': 0.353, 'comp...  \n",
              "1  {'neg': 0.116, 'neu': 0.564, 'pos': 0.32, 'com...  \n",
              "2  {'neg': 0.0, 'neu': 0.504, 'pos': 0.496, 'comp...  \n",
              "3  {'neg': 0.094, 'neu': 0.554, 'pos': 0.352, 'co...  \n",
              "4  {'neg': 0.094, 'neu': 0.643, 'pos': 0.263, 'co...  "
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_reduced['scores']=df_reduced['text_clean'].apply(lambda body: vaderSentimentAnalyzer.polarity_scores(str(body)))\n",
        "df_reduced.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num_row</th>\n",
              "      <th>text_clean</th>\n",
              "      <th>scores</th>\n",
              "      <th>compound_sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>purchased book neice cruised week loved story ...</td>\n",
              "      <td>{'neg': 0.0, 'neu': 0.647, 'pos': 0.353, 'comp...</td>\n",
              "      <td>0.9623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>really enjoyed book several front author terri...</td>\n",
              "      <td>{'neg': 0.116, 'neu': 0.564, 'pos': 0.32, 'com...</td>\n",
              "      <td>0.9904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>interesting informative spot good book read wa...</td>\n",
              "      <td>{'neg': 0.0, 'neu': 0.504, 'pos': 0.496, 'comp...</td>\n",
              "      <td>0.7096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>must learn past history better appreciate toda...</td>\n",
              "      <td>{'neg': 0.094, 'neu': 0.554, 'pos': 0.352, 'co...</td>\n",
              "      <td>0.9595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>began book high hope cover blurb sounded inter...</td>\n",
              "      <td>{'neg': 0.094, 'neu': 0.643, 'pos': 0.263, 'co...</td>\n",
              "      <td>0.9933</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   num_row                                         text_clean  \\\n",
              "0        0  purchased book neice cruised week loved story ...   \n",
              "1        1  really enjoyed book several front author terri...   \n",
              "2        2  interesting informative spot good book read wa...   \n",
              "3        3  must learn past history better appreciate toda...   \n",
              "4        4  began book high hope cover blurb sounded inter...   \n",
              "\n",
              "                                              scores  compound_sentiment  \n",
              "0  {'neg': 0.0, 'neu': 0.647, 'pos': 0.353, 'comp...              0.9623  \n",
              "1  {'neg': 0.116, 'neu': 0.564, 'pos': 0.32, 'com...              0.9904  \n",
              "2  {'neg': 0.0, 'neu': 0.504, 'pos': 0.496, 'comp...              0.7096  \n",
              "3  {'neg': 0.094, 'neu': 0.554, 'pos': 0.352, 'co...              0.9595  \n",
              "4  {'neg': 0.094, 'neu': 0.643, 'pos': 0.263, 'co...              0.9933  "
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_reduced['compound_sentiment']=df_reduced['scores'].apply(lambda score_dict:score_dict['compound'])\n",
        "df_reduced.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num_row</th>\n",
              "      <th>text_clean</th>\n",
              "      <th>compound_sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9222</th>\n",
              "      <td>9222</td>\n",
              "      <td>typical grisham book plenty action movement al...</td>\n",
              "      <td>0.7845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13637</th>\n",
              "      <td>13637</td>\n",
              "      <td>tony valentine novel james swain creates chara...</td>\n",
              "      <td>0.9776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15734</th>\n",
              "      <td>15734</td>\n",
              "      <td>georgette heyer mystery writer pleasant surpri...</td>\n",
              "      <td>0.8271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12539</th>\n",
              "      <td>12539</td>\n",
              "      <td>thesaurus laya steinberg illustrated debbie ha...</td>\n",
              "      <td>0.9750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1836</th>\n",
              "      <td>1836</td>\n",
              "      <td>interesting deep thoughtful read definitely ad...</td>\n",
              "      <td>0.7906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7484</th>\n",
              "      <td>7484</td>\n",
              "      <td>great book recommend anyone excellent book rea...</td>\n",
              "      <td>0.9153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10726</th>\n",
              "      <td>10726</td>\n",
              "      <td>perilous time great book fate free speech amer...</td>\n",
              "      <td>-0.6013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13871</th>\n",
              "      <td>13871</td>\n",
              "      <td>adventure another keep interest going definite...</td>\n",
              "      <td>0.8720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4670</th>\n",
              "      <td>4670</td>\n",
              "      <td>part remind convincing glimpse tortured mind t...</td>\n",
              "      <td>0.9805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2286</th>\n",
              "      <td>2286</td>\n",
              "      <td>northanger abbey apparently written early jane...</td>\n",
              "      <td>0.8932</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       num_row                                         text_clean  \\\n",
              "9222      9222  typical grisham book plenty action movement al...   \n",
              "13637    13637  tony valentine novel james swain creates chara...   \n",
              "15734    15734  georgette heyer mystery writer pleasant surpri...   \n",
              "12539    12539  thesaurus laya steinberg illustrated debbie ha...   \n",
              "1836      1836  interesting deep thoughtful read definitely ad...   \n",
              "7484      7484  great book recommend anyone excellent book rea...   \n",
              "10726    10726  perilous time great book fate free speech amer...   \n",
              "13871    13871  adventure another keep interest going definite...   \n",
              "4670      4670  part remind convincing glimpse tortured mind t...   \n",
              "2286      2286  northanger abbey apparently written early jane...   \n",
              "\n",
              "       compound_sentiment  \n",
              "9222               0.7845  \n",
              "13637              0.9776  \n",
              "15734              0.8271  \n",
              "12539              0.9750  \n",
              "1836               0.7906  \n",
              "7484               0.9153  \n",
              "10726             -0.6013  \n",
              "13871              0.8720  \n",
              "4670               0.9805  \n",
              "2286               0.8932  "
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_reduced = df_reduced.drop(['scores'], axis=1)\n",
        "df_reduced.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_reduced.to_csv('../data/processed/df_reduced_with_sentiment.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
